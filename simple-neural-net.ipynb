{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open(\"names.txt\", 'r').read().splitlines()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest name: 2\n",
      "Longest  name: 15\n",
      "Number of names: 32033\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shortest name: {min(len(w) for w in words)}\")\n",
    "print(f\"Longest  name: {max(len(w) for w in words)}\")\n",
    "print(f\"Number of names: {len(words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 27])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "NUM_CHARS = len(stoi.keys())\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "N = torch.zeros((NUM_CHARS, NUM_CHARS), dtype=torch.int32)\n",
    "N.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228146"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Time for a neural network approach ------\n",
    "\n",
    "# Build the training sets.\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    # Include the start and end chars for this word.\n",
    "    letters = ['.'] + list(w) + ['.']\n",
    "    # Iterate over 2 chars at once, n and n+1.  Bigram is just a pair of letters in\n",
    "    # the order they appear in words.\n",
    "    for ch1, ch2 in zip(letters, letters[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)  # The next letter is the desired value.\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num_inputs = xs.nelement()\n",
    "num_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 27])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change character IDs to a 1.0 in the proper place in an input matrix.\n",
    "import torch.nn.functional as F\n",
    "x_encoded = F.one_hot(xs, num_classes=NUM_CHARS).float()\n",
    "x_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  5, 13, 13,  1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As an example just look at a few cases.\n",
    "sample = xs[0:5]\n",
    "print(sample)\n",
    "sample_encoded = F.one_hot(sample, num_classes=NUM_CHARS).float() \n",
    "sample_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
       "         0.0791,  0.9046, -0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,\n",
       "         1.5618, -1.6261,  0.6772, -0.8404,  0.9849, -0.1484, -1.4795,  0.4483,\n",
       "        -0.0707,  2.4968,  2.4448], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "# Initialize weights for a set of neurons.\n",
    "NUM_NEURONS = 27\n",
    "W = torch.randn((NUM_CHARS, NUM_NEURONS), generator=g, requires_grad=True)\n",
    "# Initialize the gradient.\n",
    "W.grad = None\n",
    "W[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs[0]: tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
      "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
      "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.758953332901001"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the output = W . x + b  (for now b is zero so ignore)\n",
    "\n",
    "# We interperet the resulting value as the log of the counts we would\n",
    "# get if we had sampled the model some huge number of times.  In the \n",
    "# simple bigram model we ended up just literally counting the examples\n",
    "# in the training data.  Without doing that here we just interperet the\n",
    "# outputs as the log of those counts.  The log of counts is called logits.\n",
    "logits = x_encoded @ W\n",
    "\n",
    "counts = logits.exp()   # Exponentiating them gives back counts, like N above.\n",
    "probs  = counts / counts.sum(1, keepdim=True)\n",
    "# These last two lines are also called softmax.\n",
    "# At this point we have the probabiliites for every training example.  Each row\n",
    "# is an example input case, and the 27 entries along that row are the probabilities\n",
    "# of that entry being the next char that the model is predicting.\n",
    "\n",
    "# This process of going once through the network multiplying x by W is a \"forward pass\"\n",
    "print(\"probs[0]:\", probs[0])\n",
    "\n",
    "# Now we calculate the loss.  It is the negative log likelihood.\n",
    "loss = -probs[torch.arange(num_inputs), ys].log().mean()\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "input to the net:    0\n",
      "expected next char:  5\n",
      "Probability assigned to char by the model:  0.012286253273487091\n",
      "log likelihood:  -4.3992743492126465\n",
      "neg ll:  4.3992743492126465\n",
      "-----\n",
      "input to the net:    5\n",
      "expected next char:  13\n",
      "Probability assigned to char by the model:  0.018050702288746834\n",
      "log likelihood:  -4.014570713043213\n",
      "neg ll:  4.014570713043213\n",
      "-----\n",
      "input to the net:    13\n",
      "expected next char:  13\n",
      "Probability assigned to char by the model:  0.026691533625125885\n",
      "log likelihood:  -3.623408794403076\n",
      "neg ll:  3.623408794403076\n",
      "-----\n",
      "input to the net:    13\n",
      "expected next char:  1\n",
      "Probability assigned to char by the model:  0.07367684692144394\n",
      "log likelihood:  -2.6080667972564697\n",
      "neg ll:  2.6080667972564697\n",
      "-----\n",
      "input to the net:    1\n",
      "expected next char:  0\n",
      "Probability assigned to char by the model:  0.0149775305762887\n",
      "log likelihood:  -4.201204299926758\n",
      "neg ll:  4.201204299926758\n"
     ]
    }
   ],
   "source": [
    "# BTW, here is what's happening in that list comprehension.\n",
    "# for i in range(num_inputs):\n",
    "#     x = xs[i]  <-- input to network\n",
    "#     y = ys[i]  <-- expected output\n",
    "#     probability_for_next_char = probs[i, y]\n",
    "#     log_likelihood = torch.log(probability_for_next_char)\n",
    "#     neg_log_likelihood = -log_likelihood\n",
    "#     all_negll.append(neg_log_likelihood)\n",
    "# avg_neg_ll = all_negll.mean()\n",
    "for i in range(5):  # Let's just look at 5 samples.\n",
    "    x = xs[i].item()  # <-- input to network\n",
    "    y = ys[i].item()  # <-- expected output\n",
    "    print(\"-----\")\n",
    "    print(\"input to the net:   \", x)\n",
    "    print(\"expected next char: \", y)\n",
    "    # print(\"output probs of the model: \", probs[i])\n",
    "    probability_for_next_char = probs[i, y]\n",
    "    print(\"Probability assigned to char by the model: \", probability_for_next_char.item())\n",
    "    log_likelihood = torch.log(probability_for_next_char)\n",
    "    print(\"log likelihood: \", log_likelihood.item())\n",
    "    neg_log_likelihood = -log_likelihood\n",
    "    print(\"neg ll: \", neg_log_likelihood.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass.  \n",
    "loss.backward()\n",
    "\n",
    "# Note how awesome this is.  loss was calculated using lots of math operations, and \n",
    "# it started by going through the network.  torch kept track of all of the math\n",
    "# operations, so we can just call .backward() on loss to go all the way backward\n",
    "# to the start of the network and compute all of the gradients.\n",
    "\n",
    "# Now use the resulting gradient to update all of the params.\n",
    "# We will go 10% along the negative of the gradient.\n",
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.758953332901001\n",
      "3.6702592372894287\n",
      "3.591153860092163\n",
      "3.520017385482788\n",
      "3.4557948112487793\n",
      "3.3977162837982178\n",
      "3.345163583755493\n",
      "3.2975902557373047\n",
      "3.254483938217163\n",
      "3.2153544425964355\n",
      "3.17973256111145\n",
      "3.147189140319824\n",
      "3.117339849472046\n",
      "3.0898516178131104\n",
      "3.064443588256836\n",
      "3.0408785343170166\n",
      "3.0189590454101562\n",
      "2.998518705368042\n",
      "2.9794163703918457\n",
      "2.9615297317504883\n",
      "2.9447529315948486\n",
      "2.928992509841919\n",
      "2.9141645431518555\n",
      "2.900193929672241\n",
      "2.887012004852295\n",
      "2.874558925628662\n",
      "2.8627758026123047\n",
      "2.851613759994507\n",
      "2.841024398803711\n",
      "2.830965757369995\n",
      "2.8213980197906494\n",
      "2.812286138534546\n",
      "2.8035969734191895\n",
      "2.7953009605407715\n",
      "2.7873711585998535\n",
      "2.7797818183898926\n",
      "2.7725107669830322\n",
      "2.7655372619628906\n",
      "2.7588419914245605\n",
      "2.7524070739746094\n",
      "2.7462172508239746\n",
      "2.7402572631835938\n",
      "2.7345142364501953\n",
      "2.7289748191833496\n",
      "2.7236287593841553\n",
      "2.7184643745422363\n",
      "2.713472604751587\n",
      "2.708644390106201\n",
      "2.7039716243743896\n",
      "2.699446678161621\n",
      "2.6950621604919434\n",
      "2.6908113956451416\n",
      "2.6866886615753174\n",
      "2.682687997817993\n",
      "2.678804397583008\n",
      "2.675032615661621\n",
      "2.671367883682251\n",
      "2.6678061485290527\n",
      "2.6643431186676025\n",
      "2.6609747409820557\n",
      "2.657698392868042\n",
      "2.6545093059539795\n",
      "2.651404619216919\n",
      "2.6483819484710693\n",
      "2.645437479019165\n",
      "2.6425693035125732\n",
      "2.6397740840911865\n",
      "2.6370491981506348\n",
      "2.6343932151794434\n",
      "2.631803035736084\n",
      "2.6292762756347656\n",
      "2.6268115043640137\n",
      "2.624406576156616\n",
      "2.6220591068267822\n",
      "2.619767665863037\n",
      "2.617530584335327\n",
      "2.6153454780578613\n",
      "2.6132118701934814\n",
      "2.611126661300659\n",
      "2.6090891361236572\n",
      "2.60709810256958\n",
      "2.605151653289795\n",
      "2.6032485961914062\n",
      "2.6013879776000977\n",
      "2.5995678901672363\n",
      "2.5977871417999268\n",
      "2.5960450172424316\n",
      "2.5943403244018555\n",
      "2.5926713943481445\n",
      "2.5910379886627197\n",
      "2.5894384384155273\n",
      "2.58787202835083\n",
      "2.5863375663757324\n",
      "2.584834575653076\n",
      "2.5833616256713867\n",
      "2.581918239593506\n",
      "2.580503225326538\n",
      "2.5791163444519043\n",
      "2.577756404876709\n",
      "2.5764224529266357\n",
      "2.5751144886016846\n",
      "2.573831081390381\n",
      "2.5725719928741455\n",
      "2.571335792541504\n",
      "2.5701231956481934\n",
      "2.568932294845581\n",
      "2.567763566970825\n",
      "2.5666158199310303\n",
      "2.56548810005188\n",
      "2.5643808841705322\n",
      "2.5632927417755127\n",
      "2.5622241497039795\n",
      "2.561173439025879\n",
      "2.5601413249969482\n",
      "2.559126377105713\n",
      "2.5581283569335938\n",
      "2.557147264480591\n",
      "2.556182384490967\n",
      "2.5552332401275635\n",
      "2.554299831390381\n",
      "2.5533814430236816\n",
      "2.5524778366088867\n",
      "2.551588773727417\n",
      "2.5507137775421143\n",
      "2.549852132797241\n",
      "2.549004077911377\n",
      "2.548168897628784\n",
      "2.547346830368042\n",
      "2.546537399291992\n",
      "2.5457398891448975\n",
      "2.544954776763916\n",
      "2.5441811084747314\n",
      "2.5434184074401855\n",
      "2.5426676273345947\n",
      "2.5419275760650635\n",
      "2.5411980152130127\n",
      "2.5404791831970215\n",
      "2.5397706031799316\n",
      "2.539072036743164\n",
      "2.5383832454681396\n",
      "2.5377047061920166\n",
      "2.5370349884033203\n",
      "2.536375045776367\n",
      "2.535723924636841\n",
      "2.535081624984741\n",
      "2.5344483852386475\n",
      "2.533823251724243\n",
      "2.5332071781158447\n",
      "2.5325987339019775\n",
      "2.531998634338379\n",
      "2.5314066410064697\n",
      "2.5308220386505127\n",
      "2.530245780944824\n",
      "2.529676675796509\n",
      "2.5291144847869873\n",
      "2.528560161590576\n",
      "2.528012990951538\n",
      "2.527472496032715\n",
      "2.5269389152526855\n",
      "2.52641224861145\n",
      "2.5258920192718506\n",
      "2.525378465652466\n",
      "2.524871587753296\n",
      "2.5243706703186035\n",
      "2.5238759517669678\n",
      "2.5233874320983887\n",
      "2.522904634475708\n",
      "2.522428035736084\n",
      "2.5219571590423584\n",
      "2.5214920043945312\n",
      "2.5210323333740234\n",
      "2.520578145980835\n",
      "2.520129680633545\n",
      "2.519686222076416\n",
      "2.5192480087280273\n",
      "2.518815279006958\n",
      "2.5183873176574707\n",
      "2.5179643630981445\n",
      "2.5175464153289795\n",
      "2.5171332359313965\n",
      "2.5167250633239746\n",
      "2.5163211822509766\n",
      "2.5159223079681396\n",
      "2.5155279636383057\n",
      "2.5151376724243164\n",
      "2.514752149581909\n",
      "2.514370918273926\n",
      "2.513993740081787\n",
      "2.5136213302612305\n",
      "2.5132524967193604\n",
      "2.512888193130493\n",
      "2.5125277042388916\n",
      "2.5121705532073975\n",
      "2.5118184089660645\n",
      "2.5114693641662598\n",
      "2.5111241340637207\n",
      "2.5107831954956055\n",
      "2.5104455947875977\n",
      "2.5101113319396973\n",
      "2.5097811222076416\n"
     ]
    }
   ],
   "source": [
    "# Loop to do more training.  \n",
    "\n",
    "# Gradient descent\n",
    "LEARNING_RATE = 10\n",
    "\n",
    "for k in range(200):\n",
    "    x_encoded = F.one_hot(xs, num_classes=NUM_CHARS).float()\n",
    "    logits = x_encoded @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdim=True)\n",
    "    loss = -probs[torch.arange(num_inputs), ys].log().mean()\n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None  # Reset gradients before computing them again!\n",
    "    loss.backward()\n",
    "\n",
    "    # Update\n",
    "    W.data += -LEARNING_RATE * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myliena\n",
      "r\n",
      "a\n",
      "ahi\n",
      "gsammian\n",
      "n\n",
      "xxtfpcaleldfenixe\n",
      "kar\n",
      "jaranigf\n",
      "meceeja\n",
      "ty\n",
      "sbenna\n",
      "jathe\n",
      "tyzimcalajan\n",
      "ma\n",
      "vmanpcciloly\n",
      "jai\n",
      "royu\n",
      "ka\n",
      "aqamyn\n"
     ]
    }
   ],
   "source": [
    "# We can now sample from our trained model.\n",
    "\n",
    "# Generate new names using our model.\n",
    "g = torch.Generator().manual_seed(1337)\n",
    "for i in range(20):\n",
    "    out_name = []\n",
    "    ix = 0  # Start with the start char.\n",
    "    while True:\n",
    "        # p = P[ix]  # Grab this row of probabilities.\n",
    "        x_encoded = F.one_hot(torch.tensor([ix]), num_classes=NUM_CHARS).float()\n",
    "        logits = x_encoded @ W   # Predict log-counts\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(1, keepdim=True)  # Probabilities for next char.\n",
    "        # torch.multinomial will sample from a given probability.\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        if ix == 0:\n",
    "            break\n",
    "        out_name.append(itos[ix])\n",
    "    print(''.join(out_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43dd406cf07603aefb2bc1f435b01d13dd3834c8eeb71afea2303e52aded1d18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
